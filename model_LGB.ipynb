{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', '縣市', '鄉鎮市區', '路名', '土地面積', '使用分區', '移轉層次', '總樓層數', '主要用途',\n",
       "       '主要建材', '建物型態', '屋齡', '建物面積', '車位面積', '車位個數', '橫坐標', '縱坐標', '備註',\n",
       "       '主建物面積', '陽台面積', '附屬建物面積', '單價', '縣市最低單價', '縣市最高單價', '縣市平均單價',\n",
       "       '縣市單價中位數', '鄉鎮市區最低單價', '鄉鎮市區最高單價', '鄉鎮市區平均單價', '鄉鎮市區單價中位數', '該路段最低單價',\n",
       "       '該路段最高單價', '該路段平均單價', '該路段單價中位數'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 原始data\n",
    "# df = pd.read_csv('C:\\lab\\\\aigo\\\\30_Training Dataset_V2\\\\training_data.csv')\n",
    "# test_data = pd.read_csv('C:\\lab\\\\aigo\\\\30_Public Dataset_Public Sumission Template_v2\\public_dataset.csv')\n",
    "\n",
    "# 已生成部分特徵的data\n",
    "df = pd.read_csv('C:\\lab\\\\aigo\\\\30_Training Dataset_V2\\\\training_data_processed.csv')\n",
    "test_data = pd.read_csv('C:\\lab\\\\aigo\\\\30_Public Dataset_Public Sumission Template_v2\\public_dataset_processed.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 LabelEncoder 將文字資料轉換為數值\n",
    "\n",
    "def oneHotEncode(df, col_list):\n",
    "    df = pd.get_dummies(df, columns=col_list,dtype=int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def labelEncode(df, col_list):\n",
    "    for col in col_list:\n",
    "        df[col] = labelencoder.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "# 處理數值特徵的偏移值\n",
    "# 將負數進行平移，使其成為正數，並與正數保持平移前的相對關係\n",
    "def offset_cal(df, col_list):\n",
    "    for col in col_list:\n",
    "        offset = abs(min(df[col]))\n",
    "        df[col] = df[col] + offset\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用於解決因使用dummies導致資料得column不一致的狀態，為data加上不存在的col，並補上0的值\n",
    "def make_columns_consistent(data_1st, data_2nd, default_value = 0):\n",
    "    # 取得train data、test data的column name\n",
    "    train_columns = set(data_1st.columns)\n",
    "    test_columns = set(data_2nd.columns)\n",
    "\n",
    "    # 檢查兩個data中是否存在額外的column\n",
    "    missing_columns_1st = test_columns - train_columns\n",
    "    missing_columns_2nd = train_columns - test_columns\n",
    "\n",
    "    # 將存在於data_2nd 但不存在data_1st的column加入 data_1st，並將該值設為0\n",
    "    for col in missing_columns_1st:\n",
    "        data_1st[col] = default_value  # 可以根据问题需要设置不同的默认值\n",
    "    \n",
    "    # 將存在於data_1st 但不存在data_2nd的column加入 data_2nd，並將該值設為0\n",
    "    for col in missing_columns_2nd:\n",
    "        data_2nd[col] = default_value  # 可以根据问题需要设置不同的默认值\n",
    "\n",
    "    return data_1st,data_2nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train data 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_list = ['縣市', '主要用途', '主要建材', '建物型態']\n",
    "# df = oneHotEncode(df, col_list)\n",
    "\n",
    "# col_list = ['鄉鎮市區', '路名']\n",
    "all_col = ['縣市', '鄉鎮市區', '路名', '主要用途', '主要建材', '建物型態']\n",
    "df = labelEncode(df, all_col)\n",
    "\n",
    "# 處理數值特徵的偏移值\n",
    "offset_col = ['土地面積', '建物面積', '車位面積', '主建物面積', '陽台面積', '附屬建物面積']\n",
    "df = offset_cal(df, offset_col)\n",
    "\n",
    "# 將房子所除樓層進行處理\n",
    "df['樓層'] = df['移轉層次']/df['總樓層數']\n",
    "\n",
    "# 刪除ID、string的資料\n",
    "# df = df.drop(['ID', '使用分區', '備註'], axis=1)\n",
    "df = df.drop(['ID', '縣市', '鄉鎮市區','使用分區', '移轉層次', '總樓層數', '建物面積', '備註', '陽台面積', '縣市最低單價', '縣市最高單價', '縣市平均單價', '縣市單價中位數'], axis=1)\n",
    "\n",
    "x = df.drop(['單價'], axis=1)\n",
    "y = df['單價']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test data 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_list = ['縣市', '主要用途', '主要建材', '建物型態']\n",
    "# test_data = labelEncode(test_data, col_list)\n",
    "# test_data = oneHotEncode(test_data, col_list)\n",
    "\n",
    "# col_list = ['鄉鎮市區', '路名']\n",
    "all_col = ['縣市', '鄉鎮市區', '路名', '主要用途', '主要建材', '建物型態']\n",
    "test_data = labelEncode(test_data, all_col)\n",
    "\n",
    "# 處理數值特徵的偏移值\n",
    "offset_col = ['土地面積', '建物面積', '車位面積', '主建物面積', '陽台面積', '附屬建物面積']\n",
    "test_data = offset_cal(test_data, offset_col)\n",
    "\n",
    "# 將房子所除樓層進行處理\n",
    "test_data['樓層'] = test_data['移轉層次']/test_data['總樓層數']\n",
    "\n",
    "# 刪除ID、string的資料\n",
    "# test_data = test_data.drop(['ID', '使用分區', '備註'], axis=1)\n",
    "test_data = test_data.drop(['ID', '縣市', '鄉鎮市區','使用分區', '移轉層次', '總樓層數', '建物面積', '備註', '陽台面積', '縣市最低單價', '縣市最高單價', '縣市平均單價', '縣市單價中位數'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  (11751, 21)\n",
      "test_data.shape:  (5876, 21)\n"
     ]
    }
   ],
   "source": [
    "print(\"x.shape: \",x.shape)\n",
    "print(\"test_data.shape: \",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['路名', '土地面積', '主要用途', '主要建材', '建物型態', '屋齡', '車位面積', '車位個數', '橫坐標',\n",
       "       '縱坐標', '主建物面積', '附屬建物面積', '單價', '鄉鎮市區最低單價', '鄉鎮市區最高單價', '鄉鎮市區平均單價',\n",
       "       '鄉鎮市區單價中位數', '該路段最低單價', '該路段最高單價', '該路段平均單價', '該路段單價中位數', '樓層'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解決因使用dummies導致資料得column不一致的狀態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解決因使用dummies導致資料得column不一致的狀態\n",
    "x, test_data = make_columns_consistent(x, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  (11751, 21)\n",
      "x.shape:  21\n",
      "test_data.shape:  (5876, 21)\n"
     ]
    }
   ],
   "source": [
    "print(\"x.shape: \",x.shape)\n",
    "print(\"x.shape: \",x.shape[1])\n",
    "print(\"test_data.shape: \",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['路名', '土地面積', '主要用途', '主要建材', '建物型態', '屋齡', '車位面積', '車位個數', '橫坐標',\n",
       "       '縱坐標', '主建物面積', '附屬建物面積', '鄉鎮市區最低單價', '鄉鎮市區最高單價', '鄉鎮市區平均單價',\n",
       "       '鄉鎮市區單價中位數', '該路段最低單價', '該路段最高單價', '該路段平均單價', '該路段單價中位數', '樓層'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import package、自定義dataset、定義模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "\n",
    "# 自定義 MAPE 評估函數\n",
    "def mape_eval(y_pred, data):\n",
    "    y_true = data.get_label()\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return 'MAPE', mape, False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data進行transform及切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化特徵\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(x)\n",
    "\n",
    "# 將數據劃分為訓練集和測試集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('X_train ',type(X_train))\n",
    "# print('X_val ',type(X_val))\n",
    "\n",
    "# print('y_train ',type(y_train))\n",
    "# print('y_val ',type(y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看是否有gpu、定義loss及optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3612\n",
      "[LightGBM] [Info] Number of data points in the train set: 9400, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.993345\n",
      "[1]\tvalid_0's l1: 0.663902\tvalid_0's MAPE: 39.337\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's l1: 0.603997\tvalid_0's MAPE: 35.7703\n",
      "[3]\tvalid_0's l1: 0.551336\tvalid_0's MAPE: 32.6156\n",
      "[4]\tvalid_0's l1: 0.503711\tvalid_0's MAPE: 29.778\n",
      "[5]\tvalid_0's l1: 0.461076\tvalid_0's MAPE: 27.2279\n",
      "[6]\tvalid_0's l1: 0.423295\tvalid_0's MAPE: 24.962\n",
      "[7]\tvalid_0's l1: 0.389942\tvalid_0's MAPE: 22.9295\n",
      "[8]\tvalid_0's l1: 0.360188\tvalid_0's MAPE: 21.1357\n",
      "[9]\tvalid_0's l1: 0.334079\tvalid_0's MAPE: 19.5233\n",
      "[10]\tvalid_0's l1: 0.311241\tvalid_0's MAPE: 18.1157\n",
      "[11]\tvalid_0's l1: 0.29112\tvalid_0's MAPE: 16.8702\n",
      "[12]\tvalid_0's l1: 0.272994\tvalid_0's MAPE: 15.7415\n",
      "[13]\tvalid_0's l1: 0.257414\tvalid_0's MAPE: 14.7543\n",
      "[14]\tvalid_0's l1: 0.24352\tvalid_0's MAPE: 13.8911\n",
      "[15]\tvalid_0's l1: 0.23178\tvalid_0's MAPE: 13.1333\n",
      "[16]\tvalid_0's l1: 0.221733\tvalid_0's MAPE: 12.464\n",
      "[17]\tvalid_0's l1: 0.213182\tvalid_0's MAPE: 11.9073\n",
      "[18]\tvalid_0's l1: 0.205838\tvalid_0's MAPE: 11.4228\n",
      "[19]\tvalid_0's l1: 0.199198\tvalid_0's MAPE: 10.9669\n",
      "[20]\tvalid_0's l1: 0.19314\tvalid_0's MAPE: 10.5509\n",
      "[21]\tvalid_0's l1: 0.18822\tvalid_0's MAPE: 10.2038\n",
      "[22]\tvalid_0's l1: 0.183798\tvalid_0's MAPE: 9.88122\n",
      "[23]\tvalid_0's l1: 0.180322\tvalid_0's MAPE: 9.63\n",
      "[24]\tvalid_0's l1: 0.177154\tvalid_0's MAPE: 9.39731\n",
      "[25]\tvalid_0's l1: 0.174245\tvalid_0's MAPE: 9.17978\n",
      "[26]\tvalid_0's l1: 0.172021\tvalid_0's MAPE: 9.01329\n",
      "[27]\tvalid_0's l1: 0.169691\tvalid_0's MAPE: 8.85062\n",
      "[28]\tvalid_0's l1: 0.167348\tvalid_0's MAPE: 8.69905\n",
      "[29]\tvalid_0's l1: 0.16569\tvalid_0's MAPE: 8.57535\n",
      "[30]\tvalid_0's l1: 0.164268\tvalid_0's MAPE: 8.47004\n",
      "[31]\tvalid_0's l1: 0.162918\tvalid_0's MAPE: 8.37148\n",
      "[32]\tvalid_0's l1: 0.161803\tvalid_0's MAPE: 8.28224\n",
      "[33]\tvalid_0's l1: 0.160827\tvalid_0's MAPE: 8.20834\n",
      "[34]\tvalid_0's l1: 0.159836\tvalid_0's MAPE: 8.14551\n",
      "[35]\tvalid_0's l1: 0.159157\tvalid_0's MAPE: 8.09063\n",
      "[36]\tvalid_0's l1: 0.158434\tvalid_0's MAPE: 8.03942\n",
      "[37]\tvalid_0's l1: 0.157719\tvalid_0's MAPE: 7.99107\n",
      "[38]\tvalid_0's l1: 0.157232\tvalid_0's MAPE: 7.94862\n",
      "[39]\tvalid_0's l1: 0.156662\tvalid_0's MAPE: 7.91375\n",
      "[40]\tvalid_0's l1: 0.156148\tvalid_0's MAPE: 7.88244\n",
      "[41]\tvalid_0's l1: 0.155471\tvalid_0's MAPE: 7.84027\n",
      "[42]\tvalid_0's l1: 0.155279\tvalid_0's MAPE: 7.82369\n",
      "[43]\tvalid_0's l1: 0.154985\tvalid_0's MAPE: 7.79761\n",
      "[44]\tvalid_0's l1: 0.154493\tvalid_0's MAPE: 7.77037\n",
      "[45]\tvalid_0's l1: 0.154065\tvalid_0's MAPE: 7.74855\n",
      "[46]\tvalid_0's l1: 0.1539\tvalid_0's MAPE: 7.736\n",
      "[47]\tvalid_0's l1: 0.153719\tvalid_0's MAPE: 7.72397\n",
      "[48]\tvalid_0's l1: 0.1536\tvalid_0's MAPE: 7.71402\n",
      "[49]\tvalid_0's l1: 0.153391\tvalid_0's MAPE: 7.70069\n",
      "[50]\tvalid_0's l1: 0.153107\tvalid_0's MAPE: 7.68712\n",
      "[51]\tvalid_0's l1: 0.152928\tvalid_0's MAPE: 7.67795\n",
      "[52]\tvalid_0's l1: 0.152871\tvalid_0's MAPE: 7.67057\n",
      "[53]\tvalid_0's l1: 0.152852\tvalid_0's MAPE: 7.664\n",
      "[54]\tvalid_0's l1: 0.152616\tvalid_0's MAPE: 7.6563\n",
      "[55]\tvalid_0's l1: 0.152465\tvalid_0's MAPE: 7.65015\n",
      "[56]\tvalid_0's l1: 0.152334\tvalid_0's MAPE: 7.64353\n",
      "[57]\tvalid_0's l1: 0.152056\tvalid_0's MAPE: 7.6282\n",
      "[58]\tvalid_0's l1: 0.152007\tvalid_0's MAPE: 7.62661\n",
      "[59]\tvalid_0's l1: 0.151943\tvalid_0's MAPE: 7.62123\n",
      "[60]\tvalid_0's l1: 0.151737\tvalid_0's MAPE: 7.60851\n",
      "[61]\tvalid_0's l1: 0.151634\tvalid_0's MAPE: 7.60082\n",
      "[62]\tvalid_0's l1: 0.151515\tvalid_0's MAPE: 7.5922\n",
      "[63]\tvalid_0's l1: 0.151488\tvalid_0's MAPE: 7.59253\n",
      "[64]\tvalid_0's l1: 0.151224\tvalid_0's MAPE: 7.57717\n",
      "[65]\tvalid_0's l1: 0.15122\tvalid_0's MAPE: 7.57478\n",
      "[66]\tvalid_0's l1: 0.151147\tvalid_0's MAPE: 7.57271\n",
      "[67]\tvalid_0's l1: 0.151177\tvalid_0's MAPE: 7.57558\n",
      "[68]\tvalid_0's l1: 0.150971\tvalid_0's MAPE: 7.56216\n",
      "[69]\tvalid_0's l1: 0.150782\tvalid_0's MAPE: 7.55433\n",
      "[70]\tvalid_0's l1: 0.150656\tvalid_0's MAPE: 7.5495\n",
      "[71]\tvalid_0's l1: 0.150515\tvalid_0's MAPE: 7.54298\n",
      "[72]\tvalid_0's l1: 0.15044\tvalid_0's MAPE: 7.53973\n",
      "[73]\tvalid_0's l1: 0.150339\tvalid_0's MAPE: 7.53717\n",
      "[74]\tvalid_0's l1: 0.150358\tvalid_0's MAPE: 7.53657\n",
      "[75]\tvalid_0's l1: 0.150392\tvalid_0's MAPE: 7.53578\n",
      "[76]\tvalid_0's l1: 0.150219\tvalid_0's MAPE: 7.52715\n",
      "[77]\tvalid_0's l1: 0.15016\tvalid_0's MAPE: 7.52543\n",
      "[78]\tvalid_0's l1: 0.150015\tvalid_0's MAPE: 7.52017\n",
      "[79]\tvalid_0's l1: 0.149818\tvalid_0's MAPE: 7.51011\n",
      "[80]\tvalid_0's l1: 0.149777\tvalid_0's MAPE: 7.50962\n",
      "[81]\tvalid_0's l1: 0.149794\tvalid_0's MAPE: 7.51069\n",
      "[82]\tvalid_0's l1: 0.149686\tvalid_0's MAPE: 7.50478\n",
      "[83]\tvalid_0's l1: 0.149686\tvalid_0's MAPE: 7.50546\n",
      "[84]\tvalid_0's l1: 0.149703\tvalid_0's MAPE: 7.50786\n",
      "[85]\tvalid_0's l1: 0.149582\tvalid_0's MAPE: 7.50317\n",
      "[86]\tvalid_0's l1: 0.149532\tvalid_0's MAPE: 7.49986\n",
      "[87]\tvalid_0's l1: 0.149466\tvalid_0's MAPE: 7.49823\n",
      "[88]\tvalid_0's l1: 0.14929\tvalid_0's MAPE: 7.48648\n",
      "[89]\tvalid_0's l1: 0.149105\tvalid_0's MAPE: 7.47926\n",
      "[90]\tvalid_0's l1: 0.149015\tvalid_0's MAPE: 7.47548\n",
      "[91]\tvalid_0's l1: 0.148952\tvalid_0's MAPE: 7.4717\n",
      "[92]\tvalid_0's l1: 0.148884\tvalid_0's MAPE: 7.46708\n",
      "[93]\tvalid_0's l1: 0.148782\tvalid_0's MAPE: 7.46456\n",
      "[94]\tvalid_0's l1: 0.148691\tvalid_0's MAPE: 7.45924\n",
      "[95]\tvalid_0's l1: 0.148658\tvalid_0's MAPE: 7.45926\n",
      "[96]\tvalid_0's l1: 0.148582\tvalid_0's MAPE: 7.45545\n",
      "[97]\tvalid_0's l1: 0.14859\tvalid_0's MAPE: 7.46036\n",
      "[98]\tvalid_0's l1: 0.1485\tvalid_0's MAPE: 7.45826\n",
      "[99]\tvalid_0's l1: 0.148496\tvalid_0's MAPE: 7.45886\n",
      "[100]\tvalid_0's l1: 0.148455\tvalid_0's MAPE: 7.45795\n",
      "[101]\tvalid_0's l1: 0.148475\tvalid_0's MAPE: 7.45986\n",
      "[102]\tvalid_0's l1: 0.148409\tvalid_0's MAPE: 7.45658\n",
      "[103]\tvalid_0's l1: 0.148284\tvalid_0's MAPE: 7.45062\n",
      "[104]\tvalid_0's l1: 0.148152\tvalid_0's MAPE: 7.44402\n",
      "[105]\tvalid_0's l1: 0.148162\tvalid_0's MAPE: 7.44554\n",
      "[106]\tvalid_0's l1: 0.148183\tvalid_0's MAPE: 7.44749\n",
      "[107]\tvalid_0's l1: 0.147934\tvalid_0's MAPE: 7.43203\n",
      "[108]\tvalid_0's l1: 0.14793\tvalid_0's MAPE: 7.43034\n",
      "[109]\tvalid_0's l1: 0.147785\tvalid_0's MAPE: 7.42206\n",
      "[110]\tvalid_0's l1: 0.147666\tvalid_0's MAPE: 7.41724\n",
      "[111]\tvalid_0's l1: 0.147664\tvalid_0's MAPE: 7.41745\n",
      "[112]\tvalid_0's l1: 0.147546\tvalid_0's MAPE: 7.40798\n",
      "[113]\tvalid_0's l1: 0.147493\tvalid_0's MAPE: 7.40786\n",
      "[114]\tvalid_0's l1: 0.147491\tvalid_0's MAPE: 7.40826\n",
      "[115]\tvalid_0's l1: 0.147272\tvalid_0's MAPE: 7.4021\n",
      "[116]\tvalid_0's l1: 0.147194\tvalid_0's MAPE: 7.40079\n",
      "[117]\tvalid_0's l1: 0.147145\tvalid_0's MAPE: 7.39789\n",
      "[118]\tvalid_0's l1: 0.147071\tvalid_0's MAPE: 7.3928\n",
      "[119]\tvalid_0's l1: 0.147079\tvalid_0's MAPE: 7.39176\n",
      "[120]\tvalid_0's l1: 0.147013\tvalid_0's MAPE: 7.3887\n",
      "[121]\tvalid_0's l1: 0.146981\tvalid_0's MAPE: 7.38896\n",
      "[122]\tvalid_0's l1: 0.146893\tvalid_0's MAPE: 7.38568\n",
      "[123]\tvalid_0's l1: 0.146854\tvalid_0's MAPE: 7.38489\n",
      "[124]\tvalid_0's l1: 0.14687\tvalid_0's MAPE: 7.38674\n",
      "[125]\tvalid_0's l1: 0.146884\tvalid_0's MAPE: 7.38683\n",
      "[126]\tvalid_0's l1: 0.146819\tvalid_0's MAPE: 7.3854\n",
      "[127]\tvalid_0's l1: 0.146735\tvalid_0's MAPE: 7.38361\n",
      "[128]\tvalid_0's l1: 0.146799\tvalid_0's MAPE: 7.38632\n",
      "[129]\tvalid_0's l1: 0.146858\tvalid_0's MAPE: 7.38956\n",
      "[130]\tvalid_0's l1: 0.146855\tvalid_0's MAPE: 7.38882\n",
      "[131]\tvalid_0's l1: 0.146825\tvalid_0's MAPE: 7.38784\n",
      "[132]\tvalid_0's l1: 0.146731\tvalid_0's MAPE: 7.38478\n",
      "[133]\tvalid_0's l1: 0.146684\tvalid_0's MAPE: 7.38434\n",
      "[134]\tvalid_0's l1: 0.146685\tvalid_0's MAPE: 7.38659\n",
      "[135]\tvalid_0's l1: 0.146591\tvalid_0's MAPE: 7.38176\n",
      "[136]\tvalid_0's l1: 0.146689\tvalid_0's MAPE: 7.38472\n",
      "[137]\tvalid_0's l1: 0.146579\tvalid_0's MAPE: 7.37689\n",
      "[138]\tvalid_0's l1: 0.146521\tvalid_0's MAPE: 7.37599\n",
      "[139]\tvalid_0's l1: 0.146466\tvalid_0's MAPE: 7.37249\n",
      "[140]\tvalid_0's l1: 0.146449\tvalid_0's MAPE: 7.3729\n",
      "[141]\tvalid_0's l1: 0.146429\tvalid_0's MAPE: 7.37011\n",
      "[142]\tvalid_0's l1: 0.146359\tvalid_0's MAPE: 7.36844\n",
      "[143]\tvalid_0's l1: 0.146253\tvalid_0's MAPE: 7.36365\n",
      "[144]\tvalid_0's l1: 0.146248\tvalid_0's MAPE: 7.36425\n",
      "[145]\tvalid_0's l1: 0.146269\tvalid_0's MAPE: 7.36596\n",
      "[146]\tvalid_0's l1: 0.146233\tvalid_0's MAPE: 7.36465\n",
      "[147]\tvalid_0's l1: 0.146133\tvalid_0's MAPE: 7.36047\n",
      "[148]\tvalid_0's l1: 0.146072\tvalid_0's MAPE: 7.3594\n",
      "[149]\tvalid_0's l1: 0.146028\tvalid_0's MAPE: 7.3585\n",
      "[150]\tvalid_0's l1: 0.146049\tvalid_0's MAPE: 7.36002\n",
      "[151]\tvalid_0's l1: 0.145998\tvalid_0's MAPE: 7.35809\n",
      "[152]\tvalid_0's l1: 0.145981\tvalid_0's MAPE: 7.35599\n",
      "[153]\tvalid_0's l1: 0.145945\tvalid_0's MAPE: 7.3546\n",
      "[154]\tvalid_0's l1: 0.145886\tvalid_0's MAPE: 7.35252\n",
      "[155]\tvalid_0's l1: 0.145869\tvalid_0's MAPE: 7.35114\n",
      "[156]\tvalid_0's l1: 0.145869\tvalid_0's MAPE: 7.35228\n",
      "[157]\tvalid_0's l1: 0.145775\tvalid_0's MAPE: 7.34409\n",
      "[158]\tvalid_0's l1: 0.14574\tvalid_0's MAPE: 7.34446\n",
      "[159]\tvalid_0's l1: 0.14566\tvalid_0's MAPE: 7.34284\n",
      "[160]\tvalid_0's l1: 0.145614\tvalid_0's MAPE: 7.3416\n",
      "[161]\tvalid_0's l1: 0.145524\tvalid_0's MAPE: 7.33763\n",
      "[162]\tvalid_0's l1: 0.14549\tvalid_0's MAPE: 7.3378\n",
      "[163]\tvalid_0's l1: 0.145484\tvalid_0's MAPE: 7.33838\n",
      "[164]\tvalid_0's l1: 0.145422\tvalid_0's MAPE: 7.33637\n",
      "[165]\tvalid_0's l1: 0.145437\tvalid_0's MAPE: 7.33859\n",
      "[166]\tvalid_0's l1: 0.145382\tvalid_0's MAPE: 7.3374\n",
      "[167]\tvalid_0's l1: 0.145423\tvalid_0's MAPE: 7.33838\n",
      "[168]\tvalid_0's l1: 0.145506\tvalid_0's MAPE: 7.34111\n",
      "[169]\tvalid_0's l1: 0.14548\tvalid_0's MAPE: 7.33947\n",
      "[170]\tvalid_0's l1: 0.145387\tvalid_0's MAPE: 7.33521\n",
      "[171]\tvalid_0's l1: 0.145342\tvalid_0's MAPE: 7.33281\n",
      "[172]\tvalid_0's l1: 0.14522\tvalid_0's MAPE: 7.32814\n",
      "[173]\tvalid_0's l1: 0.145227\tvalid_0's MAPE: 7.32914\n",
      "[174]\tvalid_0's l1: 0.145268\tvalid_0's MAPE: 7.33202\n",
      "[175]\tvalid_0's l1: 0.145214\tvalid_0's MAPE: 7.32973\n",
      "[176]\tvalid_0's l1: 0.145216\tvalid_0's MAPE: 7.32825\n",
      "[177]\tvalid_0's l1: 0.145223\tvalid_0's MAPE: 7.33139\n",
      "[178]\tvalid_0's l1: 0.145175\tvalid_0's MAPE: 7.32937\n",
      "[179]\tvalid_0's l1: 0.145162\tvalid_0's MAPE: 7.32904\n",
      "[180]\tvalid_0's l1: 0.145182\tvalid_0's MAPE: 7.32939\n",
      "[181]\tvalid_0's l1: 0.145118\tvalid_0's MAPE: 7.32836\n",
      "[182]\tvalid_0's l1: 0.145086\tvalid_0's MAPE: 7.3303\n",
      "Early stopping, best iteration is:\n",
      "[172]\tvalid_0's l1: 0.14522\tvalid_0's MAPE: 7.32814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JUNG\\anaconda3\\envs\\env_dl\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "# 检查是否有可用的GPU，如果有，选择第一个可用的GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# model = XGBRegressor(learning_rate=0.1,)\n",
    "# model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=['rmse', 'mae'], early_stopping_rounds=10) \n",
    "\n",
    "# model = SVR(kernel='rbf', C=1.0, epsilon=0.2)\n",
    "\n",
    "# 创建 LightGBM 模型并使用自定义评估函数\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',    # MAE仅用于监控训练过程，而不是最终评估指标\n",
    "    # 其他参数\n",
    "}\n",
    "# model = lgb.LGBMRegressor()\n",
    "model = lgb.train(params, train_data, valid_sets=[valid_data], feval=mape_eval, early_stopping_rounds=10, num_boost_round=1000)\n",
    "\n",
    "# model.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用欲預測的data進行test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 這裡使用public data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化特徵\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(test_data)\n",
    "\n",
    "X_test_tensor = torch.Tensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.87207299, 1.64577512, 2.71614832, ..., 1.88089578, 3.43835145,\n",
       "       2.14727519])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用模型進行預測\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# test_predictions = test_outputs.cpu().detach().numpy()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取原始CSV文件\n",
    "result = pd.read_csv('C:\\lab\\\\aigo\\\\30_Public Dataset_Public Sumission Template_v2\\public_submission.csv')\n",
    "\n",
    "# 将NumPy数组的数据覆盖到第二列\n",
    "result['predicted_price'] = y_pred\n",
    "\n",
    "# 保存DataFrame回到CSV文件\n",
    "result.to_csv('C:\\lab\\\\aigo\\\\30_Public Dataset_Public Sumission Template_v2\\public_LightGBM_dataProcessed_labelencode_1102.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看特徵對模型的影響力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.feature_importance(importance_type='split')  # 或 'gain'\n",
    "#\n",
    "#  获取特征名称\n",
    "feature_names = x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('該路段最高單價', 723),\n",
       " ('屋齡', 598),\n",
       " ('該路段最低單價', 495),\n",
       " ('主建物面積', 364),\n",
       " ('該路段單價中位數', 348),\n",
       " ('土地面積', 337),\n",
       " ('該路段平均單價', 329),\n",
       " ('橫坐標', 316),\n",
       " ('車位面積', 275),\n",
       " ('縱坐標', 268),\n",
       " ('樓層', 263),\n",
       " ('附屬建物面積', 166),\n",
       " ('路名', 161),\n",
       " ('鄉鎮市區最低單價', 99),\n",
       " ('鄉鎮市區最高單價', 98),\n",
       " ('鄉鎮市區平均單價', 90),\n",
       " ('建物型態', 87),\n",
       " ('主要用途', 56),\n",
       " ('主要建材', 52),\n",
       " ('車位個數', 21),\n",
       " ('鄉鎮市區單價中位數', 14)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_ranking = sorted(\n",
    "    zip(feature_names, feature_importance),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "feature_importance_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExplainerError",
     "evalue": "Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 3.385044, while the model output was 3.210672. If this difference is acceptable you can set check_additivity=False to disable this check.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExplainerError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mExplainer(model, X_train)\n\u001b[0;32m      4\u001b[0m \u001b[39m# shap_values = explainer(X_val.iloc[0])         # 解释单个预测\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49mshap_values(X_val)\n",
      "File \u001b[1;32mc:\\Users\\JUNG\\anaconda3\\envs\\env_dl\\lib\\site-packages\\shap\\explainers\\_tree.py:446\u001b[0m, in \u001b[0;36mTreeExplainer.shap_values\u001b[1;34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001b[0m\n\u001b[0;32m    444\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_shap_output(phi, flat_output)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m check_additivity \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmodel_output \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 446\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_additivity(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(X))\n\u001b[0;32m    448\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\JUNG\\anaconda3\\envs\\env_dl\\lib\\site-packages\\shap\\explainers\\_tree.py:579\u001b[0m, in \u001b[0;36mTreeExplainer.assert_additivity\u001b[1;34m(self, phi, model_output)\u001b[0m\n\u001b[0;32m    577\u001b[0m         check_sum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected_value[i] \u001b[39m+\u001b[39m phi[i]\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), model_output[:,i])\n\u001b[0;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     check_sum(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpected_value \u001b[39m+\u001b[39;49m phi\u001b[39m.\u001b[39;49msum(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), model_output)\n",
      "File \u001b[1;32mc:\\Users\\JUNG\\anaconda3\\envs\\env_dl\\lib\\site-packages\\shap\\explainers\\_tree.py:573\u001b[0m, in \u001b[0;36mTreeExplainer.assert_additivity.<locals>.check_sum\u001b[1;34m(sum_val, model_output)\u001b[0m\n\u001b[0;32m    569\u001b[0m     err_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m Consider retrying with the feature_perturbation=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minterventional\u001b[39m\u001b[39m'\u001b[39m\u001b[39m option.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m err_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m This check failed because for one of the samples the sum of the SHAP values\u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[0;32m    571\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39m was \u001b[39m\u001b[39m{:f}\u001b[39;00m\u001b[39m, while the model output was \u001b[39m\u001b[39m{:f}\u001b[39;00m\u001b[39m. If this difference is acceptable\u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[0;32m    572\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39m you can set check_additivity=False to disable this check.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(sum_val[ind], model_output[ind])\n\u001b[1;32m--> 573\u001b[0m \u001b[39mraise\u001b[39;00m ExplainerError(err_msg)\n",
      "\u001b[1;31mExplainerError\u001b[0m: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 3.385044, while the model output was 3.210672. If this difference is acceptable you can set check_additivity=False to disable this check."
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "# shap_values = explainer(X_val.iloc[0])         # 解释单个预测\n",
    "shap_values = explainer.shap_values(X_val)             # 解释整个数据集的预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化解釋\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速用圖表探索資料資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用pairplot探索數字型資料之間有沒有任何趨勢\n",
    "# sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用distplot來看房價主要集中的區間\n",
    "# sns.distplot(df['單價'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用df.corr()先做出各變數間的關係係數，再用heatmap作圖\n",
    "# sns.heatmap(df.corr(),annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env_dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f28ba316627bf88a1f9b69b448ae526ef7d8280f19f394f39c02611b27d24d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
