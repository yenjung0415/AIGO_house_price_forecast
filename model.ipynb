{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', '縣市', '鄉鎮市區', '路名', '土地面積', '使用分區', '移轉層次', '總樓層數', '主要用途',\n",
       "       '主要建材', '建物型態', '屋齡', '建物面積', '車位面積', '車位個數', '橫坐標', '縱坐標', '備註',\n",
       "       '主建物面積', '陽台面積', '附屬建物面積', '單價'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('C:\\lab\\\\aigo\\\\30_Training Dataset_V2\\\\training_data.csv')\n",
    "test_data = pd.read_csv('C:\\lab\\\\aigo\\\\30_Public Dataset_Public Sumission Template_v2\\public_dataset.csv')\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 LabelEncoder 將文字資料轉換為數值\n",
    "col_list = ['縣市', '鄉鎮市區', '路名', '主要用途', '主要建材', '建物型態']\n",
    "\n",
    "def oneHotEncode(df, col_list):\n",
    "    # for col in col_list:\n",
    "        # if(df[col].dtype == np.dtype('object')):\n",
    "        #     # pandas.get_dummies 可以对分类特征进行One-Hot编码\n",
    "        #     dummies = pd.get_dummies(df[col],prefix=col)\n",
    "        #     df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "        #     # # drop the encoded column\n",
    "        #     df.drop([col],axis = 1 , inplace=True)\n",
    "        # df_encoded = pd.get_dummies(df, columns=['Color'])\n",
    "    df = pd.get_dummies(df, columns=col_list,dtype=int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def labelEncode(df, col_list):\n",
    "    for col in col_list:\n",
    "        df[col] = labelencoder.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "# 處理數值特徵的偏移值\n",
    "# 將負數進行平移，使其成為正數，並與正數保持平移前的相對關係\n",
    "def offset_cal(df, col_list):\n",
    "    for col in col_list:\n",
    "        offset = abs(min(df[col]))\n",
    "        df[col] = df[col] + offset\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用於解決因使用dummies導致資料得column不一致的狀態，為data加上不存在的col，並補上0的值\n",
    "def make_columns_consistent(data_1st, data_2nd, default_value = 0):\n",
    "    # 取得train data、test data的column name\n",
    "    train_columns = set(data_1st.columns)\n",
    "    test_columns = set(data_2nd.columns)\n",
    "\n",
    "    # 檢查兩個data中是否存在額外的column\n",
    "    missing_columns_1st = test_columns - train_columns\n",
    "    missing_columns_2nd = train_columns - test_columns\n",
    "\n",
    "    # 將存在於data_2nd 但不存在data_1st的column加入 data_1st，並將該值設為0\n",
    "    for col in missing_columns_1st:\n",
    "        data_1st[col] = default_value  # 可以根据问题需要设置不同的默认值\n",
    "    \n",
    "    # 將存在於data_1st 但不存在data_2nd的column加入 data_2nd，並將該值設為0\n",
    "    for col in missing_columns_2nd:\n",
    "        data_2nd[col] = default_value  # 可以根据问题需要设置不同的默认值\n",
    "\n",
    "    return data_1st,data_2nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train data 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['縣市', '鄉鎮市區', '路名', '主要用途', '主要建材', '建物型態']\n",
    "df = labelEncode(df, col_list)\n",
    "# df = oneHotEncode(df, col_list)\n",
    "\n",
    "\n",
    "# 處理數值特徵的偏移值\n",
    "offset_col = ['土地面積', '建物面積', '車位面積', '主建物面積', '陽台面積', '附屬建物面積']\n",
    "df = offset_cal(df, offset_col)\n",
    "\n",
    "# 刪除ID、string的資料\n",
    "df = df.drop(['ID', '使用分區', '備註'], axis=1)\n",
    "\n",
    "x = df.drop(['單價'], axis=1)\n",
    "y = df['單價']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test data 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['縣市', '鄉鎮市區', '路名', '主要用途', '主要建材', '建物型態']\n",
    "test_data = labelEncode(test_data, col_list)\n",
    "# test_data = oneHotEncode(test_data, col_list)\n",
    "\n",
    "# 處理數值特徵的偏移值\n",
    "offset_col = ['土地面積', '建物面積', '車位面積', '主建物面積', '陽台面積', '附屬建物面積']\n",
    "test_data = offset_cal(test_data, offset_col)\n",
    "\n",
    "# 刪除ID、string的資料\n",
    "test_data = test_data.drop(['ID', '使用分區', '備註'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  (11751, 18)\n",
      "test_data.shape:  (5876, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"x.shape: \",x.shape)\n",
    "print(\"test_data.shape: \",test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解決因使用dummies導致資料得column不一致的狀態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解決因使用dummies導致資料得column不一致的狀態\n",
    "# x, test_data = make_columns_consistent(x, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  (11751, 18)\n",
      "x.shape:  18\n",
      "test_data.shape:  (5876, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"x.shape: \",x.shape)\n",
    "print(\"x.shape: \",x.shape[1])\n",
    "print(\"test_data.shape: \",test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import package、自定義dataset、定義模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 linear regression\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(x.shape[1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義深度學習模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(x.shape[1], 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 定義深度學習模型\n",
    "class Net_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_v2, self).__init__()\n",
    "        self.fc1 = nn.Linear(x.shape[1], 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_v3(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear_relu1 = nn.Linear(features, 128)\n",
    "        self.linear_relu2 = nn.Linear(128, 256)\n",
    "        self.linear_relu3 = nn.Linear(256, 256)\n",
    "        self.linear_relu4 = nn.Linear(256, 256)\n",
    "        self.linear5 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y_pred = self.linear_relu1(x)\n",
    "        y_pred = nn.functional.relu(y_pred)\n",
    "\n",
    "        y_pred = self.linear_relu2(y_pred)\n",
    "        y_pred = nn.functional.relu(y_pred)\n",
    "\n",
    "        y_pred = self.linear_relu3(y_pred)\n",
    "        y_pred = nn.functional.relu(y_pred)\n",
    "\n",
    "        y_pred = self.linear_relu4(y_pred)\n",
    "        y_pred = nn.functional.relu(y_pred)\n",
    "\n",
    "        y_pred = self.linear5(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data進行transform及切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化特徵\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(x)\n",
    "\n",
    "# 將數據劃分為訓練集和測試集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('X_train ',type(X_train))\n",
    "# print('X_val ',type(X_val))\n",
    "\n",
    "# print('y_train ',type(y_train))\n",
    "# print('y_val ',type(y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看是否有gpu、定義loss及optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有可用的GPU，如果有，选择第一个可用的GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = Net_v2().to(device)\n",
    "# model = LinearRegression(x.shape[1], 1).to(device)\n",
    "# model = Net(features = x.shape[1]).to(device)\n",
    "# model = XGBRegressor()\n",
    "model = XGBClassifier()\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 給予訓練的參數、將data轉為tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 300 :   0%|                                                       | 0/147 [00:00<?, ?it/s]c:\\Users\\JUNG\\anaconda3\\envs\\env_dl\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1 / 300 :  99%|████████████████████████████████▌| 145/147 [00:01<00:00, 168.51it/s, loss=1.26]c:\\Users\\JUNG\\anaconda3\\envs\\env_dl\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([56])) that is different to the input size (torch.Size([56, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1 / 300 : 100%|██████████████████████████████████| 147/147 [00:01<00:00, 99.19it/s, loss=1.02]\n",
      "Epoch 2 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 474.19it/s, loss=0.984]\n",
      "Epoch 3 / 300 : 100%|█████████████████████████████████| 147/147 [00:00<00:00, 457.94it/s, loss=1.38]\n",
      "Epoch 4 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 446.81it/s, loss=0.512]\n",
      "Epoch 5 / 300 : 100%|█████████████████████████████████| 147/147 [00:00<00:00, 444.11it/s, loss=1.08]\n",
      "Epoch 6 / 300 : 100%|█████████████████████████████████| 147/147 [00:00<00:00, 440.12it/s, loss=1.11]\n",
      "Epoch 7 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 445.46it/s, loss=0.829]\n",
      "Epoch 8 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 448.17it/s, loss=0.666]\n",
      "Epoch 9 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 442.77it/s, loss=0.712]\n",
      "Epoch 10 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 449.54it/s, loss=0.523]\n",
      "Epoch 11 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 453.70it/s, loss=0.968]\n",
      "Epoch 12 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 455.11it/s, loss=0.686]\n",
      "Epoch 13 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 455.11it/s, loss=0.871]\n",
      "Epoch 14 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 449.54it/s, loss=1.37]\n",
      "Epoch 15 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 446.81it/s, loss=0.877]\n",
      "Epoch 16 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 442.77it/s, loss=1.29]\n",
      "Epoch 17 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 445.45it/s, loss=0.672]\n",
      "Epoch 18 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 444.11it/s, loss=0.566]\n",
      "Epoch 19 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 446.80it/s, loss=1.01]\n",
      "Epoch 20 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 433.63it/s, loss=1.49]\n",
      "Epoch 21 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 433.63it/s, loss=1.56]\n",
      "Epoch 22 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 437.50it/s, loss=0.74]\n",
      "Epoch 23 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 438.80it/s, loss=2.95]\n",
      "Epoch 24 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 389.92it/s, loss=0.731]\n",
      "Epoch 25 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 388.89it/s, loss=0.775]\n",
      "Epoch 26 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 388.89it/s, loss=1.08]\n",
      "Epoch 27 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 388.89it/s, loss=0.962]\n",
      "Epoch 28 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 384.82it/s, loss=0.944]\n",
      "Epoch 29 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 375.96it/s, loss=0.923]\n",
      "Epoch 30 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 382.81it/s, loss=0.888]\n",
      "Epoch 31 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.86it/s, loss=0.693]\n",
      "Epoch 32 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 357.66it/s, loss=0.765]\n",
      "Epoch 33 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 351.68it/s, loss=3.12]\n",
      "Epoch 34 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 362.07it/s, loss=0.617]\n",
      "Epoch 35 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 358.54it/s, loss=0.921]\n",
      "Epoch 36 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 359.41it/s, loss=1.16]\n",
      "Epoch 37 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 341.86it/s, loss=1.08]\n",
      "Epoch 38 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.07it/s, loss=0.768]\n",
      "Epoch 39 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=1.31]\n",
      "Epoch 40 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=0.892]\n",
      "Epoch 41 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=1.42]\n",
      "Epoch 42 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=1.26]\n",
      "Epoch 43 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 343.46it/s, loss=0.437]\n",
      "Epoch 44 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 344.26it/s, loss=0.772]\n",
      "Epoch 45 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 350.84it/s, loss=1.07]\n",
      "Epoch 46 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=0.748]\n",
      "Epoch 47 / 300 : 100%|█████████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=1.1]\n",
      "Epoch 48 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=0.649]\n",
      "Epoch 49 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 351.67it/s, loss=0.847]\n",
      "Epoch 50 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=0.518]\n",
      "Epoch 51 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=0.803]\n",
      "Epoch 52 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=0.855]\n",
      "Epoch 53 / 300 : 100%|█████████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=1.2]\n",
      "Epoch 54 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.711]\n",
      "Epoch 55 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 356.80it/s, loss=1.56]\n",
      "Epoch 56 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 353.36it/s, loss=1.27]\n",
      "Epoch 57 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.07it/s, loss=0.843]\n",
      "Epoch 58 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 355.07it/s, loss=1.51]\n",
      "Epoch 59 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.819]\n",
      "Epoch 60 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=0.961]\n",
      "Epoch 61 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 342.66it/s, loss=1.12]\n",
      "Epoch 62 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=1.49]\n",
      "Epoch 63 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 358.54it/s, loss=2.51]\n",
      "Epoch 64 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=0.995]\n",
      "Epoch 65 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=0.792]\n",
      "Epoch 66 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 356.80it/s, loss=0.666]\n",
      "Epoch 67 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=1.08]\n",
      "Epoch 68 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=0.78]\n",
      "Epoch 69 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.896]\n",
      "Epoch 70 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 353.37it/s, loss=1.12]\n",
      "Epoch 71 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 356.80it/s, loss=0.931]\n",
      "Epoch 72 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=1.03]\n",
      "Epoch 73 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 353.37it/s, loss=1.04]\n",
      "Epoch 74 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.876]\n",
      "Epoch 75 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=1.02]\n",
      "Epoch 76 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 351.67it/s, loss=0.652]\n",
      "Epoch 77 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.933]\n",
      "Epoch 78 / 300 : 100%|█████████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=1.3]\n",
      "Epoch 79 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=0.589]\n",
      "Epoch 80 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=0.767]\n",
      "Epoch 81 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.84it/s, loss=0.925]\n",
      "Epoch 82 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.07it/s, loss=0.638]\n",
      "Epoch 83 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 360.30it/s, loss=1.16]\n",
      "Epoch 84 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 339.49it/s, loss=1.13]\n",
      "Epoch 85 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=0.843]\n",
      "Epoch 86 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 342.66it/s, loss=1.19]\n",
      "Epoch 87 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 354.21it/s, loss=1.22]\n",
      "Epoch 88 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 360.30it/s, loss=0.83]\n",
      "Epoch 89 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.967]\n",
      "Epoch 90 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 356.80it/s, loss=1.31]\n",
      "Epoch 91 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=0.671]\n",
      "Epoch 92 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=1.13]\n",
      "Epoch 93 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=2.07]\n",
      "Epoch 94 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=3.76]\n",
      "Epoch 95 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=0.782]\n",
      "Epoch 96 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 341.86it/s, loss=1.64]\n",
      "Epoch 97 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 342.66it/s, loss=1.34]\n",
      "Epoch 98 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 351.68it/s, loss=0.664]\n",
      "Epoch 99 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=0.947]\n",
      "Epoch 100 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 353.36it/s, loss=1.48]\n",
      "Epoch 101 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=1.15]\n",
      "Epoch 102 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=0.472]\n",
      "Epoch 103 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 334.85it/s, loss=0.719]\n",
      "Epoch 104 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 358.54it/s, loss=0.899]\n",
      "Epoch 105 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=0.749]\n",
      "Epoch 106 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 355.07it/s, loss=1.3]\n",
      "Epoch 107 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 357.67it/s, loss=1.02]\n",
      "Epoch 108 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 353.37it/s, loss=1.14]\n",
      "Epoch 109 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 335.62it/s, loss=0.732]\n",
      "Epoch 110 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=0.663]\n",
      "Epoch 111 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 359.41it/s, loss=0.657]\n",
      "Epoch 112 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=1.12]\n",
      "Epoch 113 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 351.67it/s, loss=0.697]\n",
      "Epoch 114 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 347.51it/s, loss=1.13]\n",
      "Epoch 115 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=0.866]\n",
      "Epoch 116 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 350.83it/s, loss=0.798]\n",
      "Epoch 117 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 345.07it/s, loss=1.15]\n",
      "Epoch 118 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=1.11]\n",
      "Epoch 119 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=0.59]\n",
      "Epoch 120 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=0.716]\n",
      "Epoch 121 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.86it/s, loss=1.05]\n",
      "Epoch 122 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 334.09it/s, loss=1.31]\n",
      "Epoch 123 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=0.73]\n",
      "Epoch 124 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 345.07it/s, loss=1.75]\n",
      "Epoch 125 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=0.95]\n",
      "Epoch 126 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 343.46it/s, loss=0.939]\n",
      "Epoch 127 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=0.635]\n",
      "Epoch 128 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=0.923]\n",
      "Epoch 129 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 339.49it/s, loss=0.504]\n",
      "Epoch 130 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=1.22]\n",
      "Epoch 131 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 344.26it/s, loss=1.51]\n",
      "Epoch 132 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 343.46it/s, loss=0.524]\n",
      "Epoch 133 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 344.26it/s, loss=1.11]\n",
      "Epoch 134 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=1.02]\n",
      "Epoch 135 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=0.9]\n",
      "Epoch 136 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=1.28]\n",
      "Epoch 137 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=0.588]\n",
      "Epoch 138 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=1.93]\n",
      "Epoch 139 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=1.27]\n",
      "Epoch 140 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 345.07it/s, loss=0.506]\n",
      "Epoch 141 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=0.57]\n",
      "Epoch 142 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.957]\n",
      "Epoch 143 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=1.28]\n",
      "Epoch 144 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 343.46it/s, loss=0.962]\n",
      "Epoch 145 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=1.18]\n",
      "Epoch 146 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=1.01]\n",
      "Epoch 147 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=1.19]\n",
      "Epoch 148 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 351.67it/s, loss=1.32]\n",
      "Epoch 149 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 351.67it/s, loss=1.09]\n",
      "Epoch 150 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 337.93it/s, loss=1.25]\n",
      "Epoch 151 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=0.713]\n",
      "Epoch 152 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 337.93it/s, loss=0.639]\n",
      "Epoch 153 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=1.67]\n",
      "Epoch 154 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=0.716]\n",
      "Epoch 155 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 339.49it/s, loss=0.742]\n",
      "Epoch 156 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 339.49it/s, loss=4.42]\n",
      "Epoch 157 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 353.37it/s, loss=0.827]\n",
      "Epoch 158 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 353.37it/s, loss=1.46]\n",
      "Epoch 159 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=0.557]\n",
      "Epoch 160 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.07it/s, loss=1.17]\n",
      "Epoch 161 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=1.08]\n",
      "Epoch 162 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=1.21]\n",
      "Epoch 163 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=0.703]\n",
      "Epoch 164 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 351.67it/s, loss=0.875]\n",
      "Epoch 165 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.07it/s, loss=1.03]\n",
      "Epoch 166 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=3.06]\n",
      "Epoch 167 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 355.07it/s, loss=0.889]\n",
      "Epoch 168 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=0.99]\n",
      "Epoch 169 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=1.03]\n",
      "Epoch 170 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=1.24]\n",
      "Epoch 171 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=1.06]\n",
      "Epoch 172 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.83it/s, loss=1.29]\n",
      "Epoch 173 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=1.06]\n",
      "Epoch 174 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.579]\n",
      "Epoch 175 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=0.965]\n",
      "Epoch 176 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=0.809]\n",
      "Epoch 177 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 353.37it/s, loss=1.03]\n",
      "Epoch 178 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 351.67it/s, loss=0.808]\n",
      "Epoch 179 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.84it/s, loss=3.61]\n",
      "Epoch 180 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=0.624]\n",
      "Epoch 181 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 350.83it/s, loss=0.979]\n",
      "Epoch 182 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=0.907]\n",
      "Epoch 183 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=0.577]\n",
      "Epoch 184 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=0.822]\n",
      "Epoch 185 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 340.28it/s, loss=0.98]\n",
      "Epoch 186 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=0.869]\n",
      "Epoch 187 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 334.85it/s, loss=0.896]\n",
      "Epoch 188 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 340.28it/s, loss=0.854]\n",
      "Epoch 189 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 356.80it/s, loss=1.14]\n",
      "Epoch 190 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 333.33it/s, loss=0.861]\n",
      "Epoch 191 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 351.67it/s, loss=1.04]\n",
      "Epoch 192 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 343.46it/s, loss=0.904]\n",
      "Epoch 193 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=0.481]\n",
      "Epoch 194 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 351.67it/s, loss=1.15]\n",
      "Epoch 195 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 345.07it/s, loss=0.799]\n",
      "Epoch 196 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 350.83it/s, loss=0.652]\n",
      "Epoch 197 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 341.86it/s, loss=0.882]\n",
      "Epoch 198 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 339.49it/s, loss=1.29]\n",
      "Epoch 199 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=0.768]\n",
      "Epoch 200 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.746]\n",
      "Epoch 201 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=0.954]\n",
      "Epoch 202 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=1.01]\n",
      "Epoch 203 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=1.08]\n",
      "Epoch 204 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=1.22]\n",
      "Epoch 205 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=1.18]\n",
      "Epoch 206 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=1.32]\n",
      "Epoch 207 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=0.555]\n",
      "Epoch 208 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 350.84it/s, loss=0.841]\n",
      "Epoch 209 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 350.84it/s, loss=1.2]\n",
      "Epoch 210 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.646]\n",
      "Epoch 211 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 332.58it/s, loss=0.981]\n",
      "Epoch 212 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 328.13it/s, loss=0.81]\n",
      "Epoch 213 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 326.67it/s, loss=0.5]\n",
      "Epoch 214 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 353.37it/s, loss=0.978]\n",
      "Epoch 215 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=1.14]\n",
      "Epoch 216 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=0.771]\n",
      "Epoch 217 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=0.858]\n",
      "Epoch 218 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.602]\n",
      "Epoch 219 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=0.733]\n",
      "Epoch 220 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 350.84it/s, loss=0.906]\n",
      "Epoch 221 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 352.52it/s, loss=1.05]\n",
      "Epoch 222 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=0.752]\n",
      "Epoch 223 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 345.07it/s, loss=0.784]\n",
      "Epoch 224 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 355.07it/s, loss=0.726]\n",
      "Epoch 225 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=3.17]\n",
      "Epoch 226 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 288.24it/s, loss=0.662]\n",
      "Epoch 227 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 345.07it/s, loss=0.756]\n",
      "Epoch 228 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 353.37it/s, loss=0.707]\n",
      "Epoch 229 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=0.994]\n",
      "Epoch 230 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 344.26it/s, loss=1.12]\n",
      "Epoch 231 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 350.84it/s, loss=0.517]\n",
      "Epoch 232 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 341.86it/s, loss=0.759]\n",
      "Epoch 233 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 344.26it/s, loss=1.08]\n",
      "Epoch 234 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=0.773]\n",
      "Epoch 235 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 339.49it/s, loss=0.849]\n",
      "Epoch 236 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 345.07it/s, loss=0.809]\n",
      "Epoch 237 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 341.86it/s, loss=0.598]\n",
      "Epoch 238 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=1.34]\n",
      "Epoch 239 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 342.66it/s, loss=1.04]\n",
      "Epoch 240 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=0.728]\n",
      "Epoch 241 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.04it/s, loss=0.56]\n",
      "Epoch 242 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=0.964]\n",
      "Epoch 243 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 347.52it/s, loss=1.27]\n",
      "Epoch 244 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 349.17it/s, loss=0.793]\n",
      "Epoch 245 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 344.26it/s, loss=0.89]\n",
      "Epoch 246 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 340.28it/s, loss=1.55]\n",
      "Epoch 247 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=0.988]\n",
      "Epoch 248 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 340.28it/s, loss=0.568]\n",
      "Epoch 249 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 340.28it/s, loss=1.58]\n",
      "Epoch 250 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.00it/s, loss=1.23]\n",
      "Epoch 251 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 325.94it/s, loss=2.44]\n",
      "Epoch 252 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 312.77it/s, loss=0.649]\n",
      "Epoch 253 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 325.94it/s, loss=0.888]\n",
      "Epoch 254 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 337.93it/s, loss=1.57]\n",
      "Epoch 255 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 330.34it/s, loss=0.543]\n",
      "Epoch 256 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 338.71it/s, loss=0.914]\n",
      "Epoch 257 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 339.49it/s, loss=0.381]\n",
      "Epoch 258 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=0.921]\n",
      "Epoch 259 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=0.698]\n",
      "Epoch 260 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 331.83it/s, loss=0.799]\n",
      "Epoch 261 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 337.93it/s, loss=1.21]\n",
      "Epoch 262 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 337.93it/s, loss=0.768]\n",
      "Epoch 263 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=1.19]\n",
      "Epoch 264 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.07it/s, loss=1.17]\n",
      "Epoch 265 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=2.5]\n",
      "Epoch 266 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 344.26it/s, loss=1.06]\n",
      "Epoch 267 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 339.49it/s, loss=0.657]\n",
      "Epoch 268 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 343.46it/s, loss=1.07]\n",
      "Epoch 269 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 333.33it/s, loss=0.687]\n",
      "Epoch 270 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 342.66it/s, loss=0.729]\n",
      "Epoch 271 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.86it/s, loss=3.47]\n",
      "Epoch 272 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 337.16it/s, loss=1.14]\n",
      "Epoch 273 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 341.86it/s, loss=1.51]\n",
      "Epoch 274 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 328.12it/s, loss=0.557]\n",
      "Epoch 275 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 344.26it/s, loss=2.6]\n",
      "Epoch 276 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 319.57it/s, loss=0.656]\n",
      "Epoch 277 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 325.94it/s, loss=0.727]\n",
      "Epoch 278 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 360.29it/s, loss=0.535]\n",
      "Epoch 279 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 350.84it/s, loss=0.45]\n",
      "Epoch 280 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 354.21it/s, loss=0.799]\n",
      "Epoch 281 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=0.883]\n",
      "Epoch 282 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 355.93it/s, loss=1.09]\n",
      "Epoch 283 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 348.34it/s, loss=0.97]\n",
      "Epoch 284 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 349.16it/s, loss=0.747]\n",
      "Epoch 285 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 346.70it/s, loss=1.55]\n",
      "Epoch 286 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 331.83it/s, loss=0.919]\n",
      "Epoch 287 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 304.35it/s, loss=0.832]\n",
      "Epoch 288 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 345.88it/s, loss=1.38]\n",
      "Epoch 289 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 301.23it/s, loss=0.558]\n",
      "Epoch 290 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 321.66it/s, loss=0.953]\n",
      "Epoch 291 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 312.10it/s, loss=0.889]\n",
      "Epoch 292 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 337.93it/s, loss=0.952]\n",
      "Epoch 293 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 369.34it/s, loss=0.865]\n",
      "Epoch 294 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 364.76it/s, loss=0.643]\n",
      "Epoch 295 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 354.22it/s, loss=0.747]\n",
      "Epoch 296 / 300 : 100%|███████████████████████████████| 147/147 [00:00<00:00, 368.42it/s, loss=1.13]\n",
      "Epoch 297 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 365.67it/s, loss=0.863]\n",
      "Epoch 298 / 300 : 100%|████████████████████████████████| 147/147 [00:00<00:00, 381.82it/s, loss=0.8]\n",
      "Epoch 299 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 350.84it/s, loss=0.844]\n",
      "Epoch 300 / 300 : 100%|██████████████████████████████| 147/147 [00:00<00:00, 303.72it/s, loss=0.868]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# 訓練模型 - 參數\n",
    "num_epochs = 300\n",
    "batch_size = 64\n",
    "\n",
    "# 将NumPy数组转换为PyTorch Tensor\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "X_val_tensor = torch.Tensor(X_val)\n",
    "\n",
    "# 将Pandas Series转换为PyTorch Tensor\n",
    "y_train_tensor = torch.Tensor(y_train.values)\n",
    "y_val_tensor = torch.Tensor(y_val.values)\n",
    "\n",
    "\n",
    "\n",
    "# 使用自定义Dataset创建数据集\n",
    "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = CustomDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    current_loss = 0.0\n",
    "    data_loader  = tqdm(train_loader, desc=f'Epoch {epoch + 1} / {num_epochs} ', ncols=100)     # ,loss: {loss.item()}\n",
    "    for data, targets in data_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "        # 使用set_postfix来更新进度条中的显示信息\n",
    "        data_loader.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用切割的dataset來驗證模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JUNG\\anaconda3\\envs\\env_dl\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([47])) that is different to the input size (torch.Size([47, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# 用val數據評估模型\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad(): # 關掉梯度計算\n",
    "    for data, targets in val_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        val_outputs = model(data)\n",
    "        # _, pred = torch.max(val_outputs, 1)\n",
    "        # correct += (pred == targets).sum().item()\n",
    "        val_loss = criterion(val_outputs, targets)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     val_inputs = torch.Tensor(X_val)\n",
    "#     val_labels = torch.Tensor(y_val)\n",
    "#     val_outputs = model(val_inputs)\n",
    "#     val_loss = criterion(val_outputs, val_labels)\n",
    "#     print(f\"均方誤差（Mean Squared Error）: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用欲預測的data進行test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 這裡使用public data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化特徵\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(test_data)\n",
    "\n",
    "X_test_tensor = torch.Tensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(test_data)\n",
    "test_loader = DataLoader(dataset = X_test_tensor, batch_size = num , shuffle = True)\n",
    "\n",
    "# 用val數據評估模型\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad(): # 關掉梯度計算\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        test_outputs = model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.971584 ],\n",
       "       [1.9660476],\n",
       "       [1.9863964],\n",
       "       ...,\n",
       "       [1.9863796],\n",
       "       [1.9756664],\n",
       "       [1.9781502]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用模型進行預測\n",
    "test_predictions = test_outputs.cpu().detach().numpy()\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取原始CSV文件\n",
    "df = pd.read_csv('C:\\lab\\\\aigo\\\\30_Public Dataset_Public Sumission Template_v2\\public_submission.csv')\n",
    "\n",
    "# 将NumPy数组的数据覆盖到第二列\n",
    "df['predicted_price'] = test_predictions\n",
    "\n",
    "# 保存DataFrame回到CSV文件\n",
    "df.to_csv('C:\\lab\\\\aigo\\\\30_Public Dataset_Public Sumission Template_v2\\public_net2_labelencode_epcho100_batch64.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速用圖表探索資料資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用pairplot探索數字型資料之間有沒有任何趨勢\n",
    "# sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用distplot來看房價主要集中的區間\n",
    "# sns.distplot(df['單價'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用df.corr()先做出各變數間的關係係數，再用heatmap作圖\n",
    "# sns.heatmap(df.corr(),annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env_dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f28ba316627bf88a1f9b69b448ae526ef7d8280f19f394f39c02611b27d24d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
